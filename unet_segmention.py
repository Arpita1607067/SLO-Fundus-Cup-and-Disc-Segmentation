# -*- coding: utf-8 -*-
"""Unet segmention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B8A9Bw5lMS2u2SLl-r7gmzL4zezLQn16
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from copy import deepcopy

def gen_cmap(N=10000):

    return matplotlib.colors.LinearSegmentedColormap.from_list("", [(0,'black'), (0.06,'blue'),
                                                                  (0.23, '#2ab6c6'), (0.38,'yellow'),
                                                                  (0.6,'red'), (1,'white')], N=N)



def plot_2dmap(img, show_colorbar=True, show_cup=False, with_ref=False, ref_img=None, delartifact=False, cm=None, title_on=True):
    if cm == None:
        cm = gen_cmap(256)

    img_copy2d = deepcopy(img)
    # mark the rim and cup regions by -1 and -2 locations
    if show_cup:
        img_copy2d[img_copy2d==-1] = np.nan
        cm.set_bad("gray")
        cm.set_under(color='lightgray')
    # mark rim and cup regions from reference img
    if with_ref:
        img_copy2d[ref_img==-2] = -2
        img_copy2d[ref_img==-1] = np.nan
        cm.set_bad(color="gray")
        cm.set_under(color='lightgray')
    # delete artifact locations defined by <=30 and >=0
    if delartifact:
        img_copy2d[(img_copy2d<=30) & (img_copy2d>0)] = 0
    fig = plt.figure()
    ax = plt.subplot(111)
    img_copy2d = ax.imshow(img_copy2d, cmap=cm, vmin=-0.00000001, vmax=350)
    if show_colorbar:
        cbar = plt.colorbar(img_copy2d, pad=0.01, aspect=12, location='left')
        cbar.set_ticks([0, 175,350])
        cbar.ax.set_yticklabels(['0 μm', '175 μm', '350 μm'])
        cbar.ax.tick_params(labelsize=14)
        if title_on:
            ax.set_title('SLO Fundus Thickness Map', fontsize=15)
        ax.axis('off')
    else:
        ax.axis('off')
    plt.show()

import os
import numpy as np
import tensorflow as tf
import pandas as pd
from tensorflow.keras.models import Model
# from content.drive.MyDrive.project_demo_materials.utils import *
import random
import cv2

# tf.random.set_seed(1)
# np.random.seed(0)
# random.seed(0)

# !pip install matplotlib

b = np.load('/content/drive/MyDrive/project_demo_materials/Problem_6_SLO_Fundus_Cup_and_Disc_Segmentation/FairSeg/Training/data_00001.npz')

d = dict(zip(("data1{}".format(k) for k in b), (b[k] for k in b)))
print(d)

d.keys()

d['data1disc_cup_mask']

d["data1slo_fundus"]

"""## 1. load data"""

image_path = '/content/drive/MyDrive/project_demo_materials/Problem_6_SLO_Fundus_Cup_and_Disc_Segmentation'

data_summary = pd.read_csv(os.path.join(image_path, 'data_summary.csv'))
data_summary

flist_train = os.listdir(os.path.join(image_path, 'FairSeg/Training'))

len(flist_train)

# len(flist_train), len(flist_test)

sample_data = np.load(os.path.join(image_path, 'FairSeg/Training', flist_train[0]))
for k in sample_data.keys():
    print(k)

plot_2dmap(sample_data['disc_cup_mask'], show_cup=True)

np.unique(sample_data['disc_cup_mask'])

sample_data['disc_cup_mask'].shape

num_classes = 3

a = -sample_data['disc_cup_mask']

one_hot = (np.arange(a.max()+1) == a[...,None]).astype(int)

print(a.shape)
print(one_hot.shape)

plot_2dmap(one_hot*255)

"""### Data loader"""

class DataGenerator(tf.keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, flist, data_path, batch_size=8, dim=(256,256), n_channels=3, shuffle=False, classes = 3):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.list_IDs = flist
        self.data_path = data_path
        self.n_channels = n_channels
        self.shuffle = shuffle
        self.on_epoch_end()
        self.classes = classes

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        # Find list of IDs
        list_IDs_temp = [self.list_IDs[k] for k in indexes]
        # Generate data
        X, y = self.__data_generation(list_IDs_temp)

        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        X = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.uint8)
        y = np.empty((self.batch_size, *self.dim, self.classes), dtype=np.int32)
        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            data = np.load(os.path.join(self.data_path, ID))
            img = cv2.resize(data['slo_fundus'], (224,224))
#             img[img>350] = 350
#             img[img<0] = 0
#             img = img / 350.
            X[i,] =  np.transpose(np.array([img, img, img]), (1, 2, 0))
            mask_img = cv2.resize(-1 * data['disc_cup_mask'], (224,224))
            y[i] = (np.arange(mask_img.max()+1) == mask_img[...,None]).astype(int)

        return X, y

"""### Test the data loader"""

image_folder = os.path.join(image_path, 'FairSeg/Training')

_generator = DataGenerator(flist_train, image_folder,
                           batch_size=4, dim=(224,224),
                           n_channels=3, shuffle=True,
                           classes = 3)

# !pip install opencv-python

imgs, labs = _generator.__getitem__(0)

imgs.shape, labs.shape

np.unique(labs)

plot_2dmap(imgs[0])

plot_2dmap(labs[0]*128)

"""## 2. Define the deep learning model"""

from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout



def build_model_efficient(IMAGE_HEIGHT=224, IMAGE_WIDTH = 224, IMAGE_CHANNELS=3, NUM_CLASSES = 3):

    inputs = tf.keras.layers.Input((IMAGE_HEIGHT, IMAGE_HEIGHT, IMAGE_CHANNELS), dtype= 'float32')

    #First label
    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)
    c1 = tf.keras.layers.Dropout(.1)(c1)
    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)
    p1 = tf.keras.layers.MaxPooling2D((2,2))(c1)


    #Secend label
    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)
    c2 = tf.keras.layers.Dropout(.1)(c2)
    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)
    p2 = tf.keras.layers.MaxPooling2D((2,2))(c2)


    #Third label
    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)
    c3 = tf.keras.layers.Dropout(.2)(c3)
    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)
    p3 = tf.keras.layers.MaxPooling2D((2,2))(c3)


    #Forth label
    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)
    c4 = tf.keras.layers.Dropout(.2)(c4)
    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)
    p4 = tf.keras.layers.MaxPooling2D((2,2))(c4)


    #Fifth label
    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)
    c5 = tf.keras.layers.Dropout(.3)(c5)
    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)



    # Reverse Path
    # sixth label
    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2,2), padding='same')(c5)
    u6 = tf.keras.layers.concatenate([u6, c4])
    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)
    c6 = tf.keras.layers.Dropout(.2)(c6)
    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)


    #Seventh label
    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2,2), padding='same')(c6)
    u7 = tf.keras.layers.concatenate([u7, c3])
    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)
    c7 = tf.keras.layers.Dropout(.2)(c7)
    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)

    #Eight label
    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2,2), padding='same')(c7)
    u8 = tf.keras.layers.concatenate([u8, c2])
    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)
    c8 = tf.keras.layers.Dropout(.1)(c8)
    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)

    #Ninth label
    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2,2), padding='same')(c8)
    u9 = tf.keras.layers.concatenate([u9, c1])
    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)
    c9 = tf.keras.layers.Dropout(.1)(c9)
    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)

    outputs = tf.keras.layers.Conv2D(NUM_CLASSES,(1,1), activation='softmax')(c9)


    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])
    return model

model = build_model_efficient()

tf.keras.utils.plot_model(model, show_shapes = True)

model.summary()

"""## 3. Train the model"""

batch_size = 16

train_generator = DataGenerator(flist_train, image_folder,
                                   batch_size=batch_size, dim=(224,224),
                                   n_channels=3, shuffle=True,
                                classes = 3)

model = build_model_efficient()
optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00001)
model.compile(optimizer = optimizer, loss = 'binary_crossentropy',
                                     metrics = ['acc', tf.keras.metrics.AUC()])

history = model.fit(train_generator,
                    steps_per_epoch = int(len(flist_train)/batch_size),
                    epochs = 10)

"""## 4. Evaluate the model"""

import tensorflow.keras.backend as K

def dice_coef(y_true, y_pred, smooth=1):
    y_true_f = tf.reshape(tf.dtypes.cast(y_true, tf.float32), [-1])
    y_pred_f = tf.reshape(tf.dtypes.cast(y_pred, tf.float32), [-1])
    intersection = K.sum(K.abs(y_true_f * y_pred_f), axis=-1)

    return (2 * intersection + smooth) / (K.sum(K.square(y_true_f),-1) + K.sum(K.square(y_pred_f),-1) + smooth)

path_attr_map = {}
for index, row in data_summary.iterrows():
    path_attr_map[row['filename']] = [row['race'], row['ethnicity'], row['gender']]

image_folder = os.path.join(image_path, 'FairSeg/Testing')
flist_test = os.listdir(os.path.join(image_path, 'FairSeg/Testing'))
test_generator = DataGenerator(flist_test, image_folder,
                               batch_size=batch_size, dim=(224,224),
                               n_channels=3, shuffle=True)

test_paths = []
test_attrs = []

overall_dice = []
asian_dice = []
black_dice = []
white_dice = []
hispanic_dice=[]
non_hispanic_dice =[]
male_dice = []
female_dice = []

indexes = np.arange(len(flist_test))
for i in range(test_generator.__len__()):
    batch_X, batch_y = test_generator.__getitem__(i)
    pred_batch_y = model.predict(batch_X, verbose=0)
    test_y= batch_y
    pred_y = np.squeeze(pred_batch_y)
    details = [path_attr_map[flist_test[idx]] for idx in indexes[i*batch_size:(i+1)*batch_size]]

    test_paths.extend([flist_test[idx] for idx in indexes[i*batch_size:(i+1)*batch_size]])
    test_attrs.extend([path_attr_map[flist_test[idx]] for idx in indexes[i*batch_size:(i+1)*batch_size]])

    for ind in range(len(pred_y)):
      ind_val = tf.convert_to_tensor(tf.where(pred_y[ind] > .5, 1, 0))
      dice_val = dice_coef(test_y[ind], ind_val)
      overall_dice.append(dice_val)
      if details[ind][0] == 'asian':
        asian_dice.append(dice_val)
      elif details[ind][0] == 'black':
        black_dice.append(dice_val)
      elif details[ind][0] == 'white':
        white_dice.append(dice_val)


      if details[ind][1] == 'hispanic':
        hispanic_dice.append(dice_val)
      elif details[ind][1] == 'non-hispanic':
        non_hispanic_dice.append(dice_val)

      if details[ind][2] == 'male':
        male_dice.append(dice_val)
      elif details[ind][2] == 'female':
        female_dice.append(dice_val)

# print(overall_dice[0])

print("Avg Dice score: ", sum(overall_dice)/len(overall_dice))
print("Asian Dice score: ", sum(asian_dice)/len(asian_dice))
print("Black Dice score: ", sum(black_dice)/len(black_dice))
print("White Dice score: ", sum(white_dice)/len(white_dice))

print("Hispanic Dice score: ", sum(hispanic_dice)/len(hispanic_dice))
print("Non-hispanic Dice score: ", sum(non_hispanic_dice)/len(non_hispanic_dice))

print("Male Dice score: ", sum(male_dice)/len(male_dice))
print("Female Dice score: ", sum(female_dice)/len(female_dice))


# # perfs = eval_test(test_y, pred_y, test_attrs)

"""### Overall performance"""

model.save(f"results/slo_segmentation.h5")
np.savez(f"results/slo_segmentation.npz",
         test_y=test_y,
         test_pred=pred_y,
         test_attrs=test_attrs,
         test_paths=test_paths)

# tf.keras.models.load_model('results/glaucoma_pred_model.h5')

