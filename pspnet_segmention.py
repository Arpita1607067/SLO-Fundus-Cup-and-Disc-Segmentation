# -*- coding: utf-8 -*-
"""PSPNet segmention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YlVVg8yEBPl35COBnQMkavlzqRKIDqzf
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from copy import deepcopy

def gen_cmap(N=10000):

    return matplotlib.colors.LinearSegmentedColormap.from_list("", [(0,'black'), (0.06,'blue'),
                                                                  (0.23, '#2ab6c6'), (0.38,'yellow'),
                                                                  (0.6,'red'), (1,'white')], N=N)



def plot_2dmap(img, show_colorbar=True, show_cup=False, with_ref=False, ref_img=None, delartifact=False, cm=None, title_on=True):
    if cm == None:
        cm = gen_cmap(256)

    img_copy2d = deepcopy(img)
    # mark the rim and cup regions by -1 and -2 locations
    if show_cup:
        img_copy2d[img_copy2d==-1] = np.nan
        cm.set_bad("gray")
        cm.set_under(color='lightgray')
    # mark rim and cup regions from reference img
    if with_ref:
        img_copy2d[ref_img==-2] = -2
        img_copy2d[ref_img==-1] = np.nan
        cm.set_bad(color="gray")
        cm.set_under(color='lightgray')
    # delete artifact locations defined by <=30 and >=0
    if delartifact:
        img_copy2d[(img_copy2d<=30) & (img_copy2d>0)] = 0
    fig = plt.figure()
    ax = plt.subplot(111)
    img_copy2d = ax.imshow(img_copy2d, cmap=cm, vmin=-0.00000001, vmax=350)
    if show_colorbar:
        cbar = plt.colorbar(img_copy2d, pad=0.01, aspect=12, location='left')
        cbar.set_ticks([0, 175,350])
        cbar.ax.set_yticklabels(['0 μm', '175 μm', '350 μm'])
        cbar.ax.tick_params(labelsize=14)
        if title_on:
            ax.set_title('SLO Fundus Thickness Map', fontsize=15)
        ax.axis('off')
    else:
        ax.axis('off')
    plt.show()

import os
import numpy as np
import tensorflow as tf
import pandas as pd
from tensorflow.keras.models import Model
# from content.drive.MyDrive.project_demo_materials.utils import *
import random
import cv2

# tf.random.set_seed(1)
# np.random.seed(0)
# random.seed(0)

# !pip install matplotlib

b = np.load('/content/drive/MyDrive/project_demo_materials/Problem_6_SLO_Fundus_Cup_and_Disc_Segmentation/FairSeg/Training/data_00001.npz')

d = dict(zip(("data1{}".format(k) for k in b), (b[k] for k in b)))
print(d)

d.keys()

d['data1disc_cup_mask']

d["data1slo_fundus"]

"""## 1. load data"""

image_path = '/content/drive/MyDrive/project_demo_materials/Problem_6_SLO_Fundus_Cup_and_Disc_Segmentation'

data_summary = pd.read_csv(os.path.join(image_path, 'data_summary.csv'))
data_summary

flist_train = os.listdir(os.path.join(image_path, 'FairSeg/Training'))

len(flist_train)

# len(flist_train), len(flist_test)

sample_data = np.load(os.path.join(image_path, 'FairSeg/Training', flist_train[0]))
for k in sample_data.keys():
    print(k)

plot_2dmap(sample_data['disc_cup_mask'], show_cup=True)

np.unique(sample_data['disc_cup_mask'])

sample_data['disc_cup_mask'].shape

num_classes = 3

a = -sample_data['disc_cup_mask']

one_hot = (np.arange(a.max()+1) == a[...,None]).astype(int)

print(a.shape)
print(one_hot.shape)

plot_2dmap(one_hot*255)

"""### Data loader"""

class DataGenerator(tf.keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, flist, data_path, batch_size=8, dim=(256,256), n_channels=3, shuffle=False, classes = 3):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.list_IDs = flist
        self.data_path = data_path
        self.n_channels = n_channels
        self.shuffle = shuffle
        self.on_epoch_end()
        self.classes = classes

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        # Find list of IDs
        list_IDs_temp = [self.list_IDs[k] for k in indexes]
        # Generate data
        X, y = self.__data_generation(list_IDs_temp)

        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        X = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.uint8)
        y = np.empty((self.batch_size, *self.dim, self.classes), dtype=np.int32)
        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            data = np.load(os.path.join(self.data_path, ID))
            img = cv2.resize(data['slo_fundus'], (256,256))
#             img[img>350] = 350
#             img[img<0] = 0
#             img = img / 350.
            X[i,] =  np.transpose(np.array([img, img, img]), (1, 2, 0))
            mask_img = cv2.resize(-1 * data['disc_cup_mask'], (256,256))
            y[i] = (np.arange(mask_img.max()+1) == mask_img[...,None]).astype(int)

        return X, y

"""### Test the data loader"""

image_folder = os.path.join(image_path, 'FairSeg/Training')

_generator = DataGenerator(flist_train, image_folder,
                           batch_size=4, dim=(256,256),
                           n_channels=3, shuffle=True,
                           classes = 3)

# !pip install opencv-python

imgs, labs = _generator.__getitem__(0)

imgs.shape, labs.shape

np.unique(labs)

plot_2dmap(imgs[0])

plot_2dmap(labs[0]*128)

"""## 2. Define the deep learning model"""

import numpy as np
import time
import os
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
from tensorflow import keras
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Convolution2D, BatchNormalization, ReLU, LeakyReLU, Add, Activation
from tensorflow.keras.layers import GlobalAveragePooling2D, AveragePooling2D, UpSampling2D

def convolutional_block(input_tensor, filters, block_identifier):
    # Dilated convolution block
    block_name = 'block_' + str(block_identifier) + '_'
    filter1, filter2, filter3 = filters
    skip_connection = input_tensor

    # Block A
    input_tensor = Convolution2D(filters=filter1, kernel_size=(1, 1), dilation_rate=(1, 1),
                      padding='same', kernel_initializer='he_normal', name=block_name + 'a')(input_tensor)
    input_tensor = BatchNormalization(name=block_name + 'batch_norm_a')(input_tensor)
    input_tensor = LeakyReLU(alpha=0.2, name=block_name + 'leakyrelu_a')(input_tensor)

    # Block B
    input_tensor = Convolution2D(filters=filter2, kernel_size=(3, 3), dilation_rate=(2, 2),
                      padding='same', kernel_initializer='he_normal', name=block_name + 'b')(input_tensor)
    input_tensor = BatchNormalization(name=block_name + 'batch_norm_b')(input_tensor)
    input_tensor = LeakyReLU(alpha=0.2, name=block_name + 'leakyrelu_b')(input_tensor)

    # Block C
    input_tensor = Convolution2D(filters=filter3, kernel_size=(1, 1), dilation_rate=(1, 1),
                      padding='same', kernel_initializer='he_normal', name=block_name + 'c')(input_tensor)
    input_tensor = BatchNormalization(name=block_name + 'batch_norm_c')(input_tensor)

    # Skip convolutional block for residual
    skip_connection = Convolution2D(filters=filter3, kernel_size=(3, 3), padding='same', name=block_name + 'skip_conv')(skip_connection)
    skip_connection = BatchNormalization(name=block_name + 'batch_norm_skip_conv')(skip_connection)

    # Block C + Skip Convolution
    input_tensor = Add(name=block_name + 'add')([input_tensor, skip_connection])
    input_tensor = ReLU(name=block_name + 'relu')(input_tensor)
    return input_tensor

def base_convolutional_block(input_layer):
    # Base convolutional block to obtain input image feature maps

    # Base Block 1
    base_result = convolutional_block(input_layer, [32, 32, 64], '1')

    # Base Block 2
    base_result = convolutional_block(base_result, [64, 64, 128], '2')

    # Base Block 3
    base_result = convolutional_block(base_result, [128, 128, 256], '3')

    return base_result

def pyramid_pooling_module(input_layer):
    # Pyramid pooling module
    base_result = base_convolutional_block(input_layer)

    # Red Pixel Pooling
    red_result = GlobalAveragePooling2D(name='red_pool')(base_result)
    red_result = tf.keras.layers.Reshape((1, 1, 256))(red_result)
    red_result = Convolution2D(filters=64, kernel_size=(1, 1), name='red_1_by_1')(red_result)
    red_result = UpSampling2D(size=256, interpolation='bilinear', name='red_upsampling')(red_result)

    # Yellow Pixel Pooling
    yellow_result = AveragePooling2D(pool_size=(2, 2), name='yellow_pool')(base_result)
    yellow_result = Convolution2D(filters=64, kernel_size=(1, 1), name='yellow_1_by_1')(yellow_result)
    yellow_result = UpSampling2D(size=2, interpolation='bilinear', name='yellow_upsampling')(yellow_result)

    # Blue Pixel Pooling
    blue_result = AveragePooling2D(pool_size=(4, 4), name='blue_pool')(base_result)
    blue_result = Convolution2D(filters=64, kernel_size=(1, 1), name='blue_1_by_1')(blue_result)
    blue_result = UpSampling2D(size=4, interpolation='bilinear', name='blue_upsampling')(blue_result)

    # Green Pixel Pooling
    green_result = AveragePooling2D(pool_size=(8, 8), name='green_pool')(base_result)
    green_result = Convolution2D(filters=64, kernel_size=(1, 1), name='green_1_by_1')(green_result)
    green_result = UpSampling2D(size=8, interpolation='bilinear', name='green_upsampling')(green_result)

    # Final Pyramid Pooling
    return tf.keras.layers.concatenate([base_result, red_result, yellow_result, blue_result, green_result])

def pyramid_based_conv(input_layer):
    result = pyramid_pooling_module(input_layer)
    result = Convolution2D(filters=num_classes, kernel_size=3, padding='same', name='last_conv_3_by_3')(result)
    result = BatchNormalization(name='last_conv_3_by_3_batch_norm')(result)
    result = Activation('sigmoid', name='last_conv_relu')(result)
    # result = tf.keras.layers.Flatten(name='last_conv_flatten')(result)
    return result

input_layer = tf.keras.Input(shape=((256, 256, 3)), name='input')
output_layer = pyramid_based_conv(input_layer)
model = Model(input_layer, output_layer)

# model = build_model_efficient()

tf.keras.utils.plot_model(model, show_shapes = True)

model.summary()

"""## 3. Train the model"""

batch_size = 16

train_generator = DataGenerator(flist_train, image_folder,
                                   batch_size=batch_size, dim=(256,256),
                                   n_channels=3, shuffle=True,
                                classes = 3)

# model = build_model_efficient()
optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00001)
model.compile(optimizer = optimizer, loss = 'binary_crossentropy',
                                     metrics = ['acc', tf.keras.metrics.AUC()])

history = model.fit(train_generator,
                    steps_per_epoch = int(len(flist_train)/batch_size),
                    epochs = 10)

"""## 4. Evaluate the model"""

import tensorflow.keras.backend as K

def dice_coef(y_true, y_pred, smooth=1):
    y_true_f = tf.reshape(tf.dtypes.cast(y_true, tf.float32), [-1])
    y_pred_f = tf.reshape(tf.dtypes.cast(y_pred, tf.float32), [-1])
    intersection = K.sum(K.abs(y_true_f * y_pred_f), axis=-1)

    return (2 * intersection + smooth) / (K.sum(K.square(y_true_f),-1) + K.sum(K.square(y_pred_f),-1) + smooth)

path_attr_map = {}
for index, row in data_summary.iterrows():
    path_attr_map[row['filename']] = [row['race'], row['ethnicity'], row['gender']]

image_folder = os.path.join(image_path, 'FairSeg/Testing')
flist_test = os.listdir(os.path.join(image_path, 'FairSeg/Testing'))
test_generator = DataGenerator(flist_test, image_folder,
                               batch_size=batch_size, dim=(256,256),
                               n_channels=3, shuffle=True)

test_paths = []
test_attrs = []

overall_dice = []
asian_dice = []
black_dice = []
white_dice = []
hispanic = []
non_hispanic =[]
male = []
female = []

indexes = np.arange(len(flist_test))
for i in range(test_generator.__len__()):
    batch_X, batch_y = test_generator.__getitem__(i)
    pred_batch_y = model.predict(batch_X, verbose=0)
    test_y= batch_y
    pred_y = np.squeeze(pred_batch_y)
    details = [path_attr_map[flist_test[idx]] for idx in indexes[i*batch_size:(i+1)*batch_size]]

    test_paths.extend([flist_test[idx] for idx in indexes[i*batch_size:(i+1)*batch_size]])
    test_attrs.extend([path_attr_map[flist_test[idx]] for idx in indexes[i*batch_size:(i+1)*batch_size]])

    for ind in range(len(pred_y)):
      ind_val = tf.convert_to_tensor(tf.where(pred_y[ind] > .5, 1, 0))
      dice_val = dice_coef(test_y[ind], ind_val)
      overall_dice.append(dice_val)
      if details[ind][0] == 'asian':
        asian_dice.append(dice_val)
      elif details[ind][0] == 'black':
        black_dice.append(dice_val)
      elif details[ind][0] == 'white':
        white_dice.append(dice_val)

      if details[ind][1] == 'hispanic':
        hispanic.append(dice_val)
      elif details[ind][1] == 'non-hispanic':
        non_hispanic.append(dice_val)

      if details[ind][2] == 'male':
        male.append(dice_val)
      elif details[ind][2] == 'female':
        female.append(dice_val)


# print(overall_dice[0])

print("Avg Dice score: ", sum(overall_dice)/len(overall_dice))
print("Asian Dice score: ", sum(asian_dice)/len(asian_dice))
print("Black Dice score: ", sum(black_dice)/len(black_dice))
print("White Dice score: ", sum(white_dice)/len(white_dice))

print("Hispanic Dice score: ", sum(hispanic)/len(hispanic))
print("Non-hispanic Dice score: ", sum(non_hispanic)/len(non_hispanic))

print("Male Dice score: ", sum(male)/len(male))
print("Female Dice score: ", sum(female)/len(female))

# # perfs = eval_test(test_y, pred_y, test_attrs)

"""### Overall performance"""

model.save(f"results/segmentation_model.h5")
np.savez(f"results/segmentation_model.npz",
         test_y=test_y,
         test_pred=pred_y,
         test_attrs=test_attrs,
         test_paths=test_paths)

# tf.keras.models.load_model('results/glaucoma_pred_model.h5')

